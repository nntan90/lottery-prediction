"""
XSMB Crawler - Crawl k·∫øt qu·∫£ x·ªï s·ªë mi·ªÅn B·∫Øc
Ngu·ªìn: xskt.com.vn (c√≥ th·ªÉ thay ƒë·ªïi)
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, date
from typing import Optional, Dict, List
import time


class XSMBCrawler:
    """Crawler cho x·ªï s·ªë mi·ªÅn B·∫Øc"""
    
    # Ngu·ªìn ch√≠nh - c√≥ th·ªÉ thay ƒë·ªïi n·∫øu website thay ƒë·ªïi c·∫•u tr√∫c
    BASE_URL = "https://xskt.com.vn/xsmb"
    
    # Ngu·ªìn d·ª± ph√≤ng
    BACKUP_URL = "https://www.minhngoc.net.vn/xo-so-mien-bac"
    
    def __init__(self):
        """Initialize crawler v·ªõi headers"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                         'AppleWebKit/537.36 (KHTML, like Gecko) '
                         'Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
        }
    
    def fetch_results(self, target_date: date) -> Optional[Dict]:
        """
        Crawl k·∫øt qu·∫£ XSMB cho ng√†y c·ª• th·ªÉ
        
        Args:
            target_date: Ng√†y c·∫ßn crawl (date object)
        
        Returns:
            Dictionary ch·ª©a k·∫øt qu·∫£ ho·∫∑c None n·∫øu failed
            {
                'draw_date': date,
                'region': 'XSMB',
                'special_prize': '12345',
                'first_prize': '67890',
                'second_prize': ['11111', '22222'],
                ...
            }
        """
        print(f"üîç Crawling XSMB for {target_date}...")
        
        try:
            # Th·ª≠ ngu·ªìn ch√≠nh tr∆∞·ªõc
            results = self._crawl_from_xskt(target_date)
            
            if results:
                print(f"‚úÖ Successfully crawled from xskt.com.vn")
                return results
            
            # N·∫øu fail, th·ª≠ ngu·ªìn d·ª± ph√≤ng
            print(f"‚ö†Ô∏è Primary source failed, trying backup...")
            results = self._crawl_from_minhngoc(target_date)
            
            if results:
                print(f"‚úÖ Successfully crawled from minhngoc.net.vn")
                return results
            
            print(f"‚ùå All sources failed for {target_date}")
            return None
            
        except Exception as e:
            print(f"‚ùå Error crawling XSMB: {e}")
            return None
    
    def _crawl_from_xskt(self, target_date: date) -> Optional[Dict]:
        """
        Crawl t·ª´ xskt.com.vn
        
        L∆ØU √ù: C·∫•u tr√∫c HTML c√≥ th·ªÉ thay ƒë·ªïi!
        N·∫øu crawler kh√¥ng ho·∫°t ƒë·ªông, c·∫ßn update selectors
        """
        try:
            # Format URL: https://xskt.com.vn/xsmb/dd-mm-yyyy.html
            date_str = target_date.strftime("%d-%m-%Y")
            url = f"{self.BASE_URL}/{date_str}.html"
            
            print(f"  ‚Üí Fetching: {url}")
            
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Parse k·∫øt qu·∫£
            # L∆ØU √ù: Selectors n√†y l√† v√≠ d·ª•, c·∫ßn ki·ªÉm tra c·∫•u tr√∫c th·ª±c t·∫ø
            results = {
                'draw_date': target_date,
                'region': 'XSMB',
                'special_prize': self._extract_prize(soup, 'special'),
                'first_prize': self._extract_prize(soup, 'first'),
                'second_prize': self._extract_prize_array(soup, 'second', 2),
                'third_prize': self._extract_prize_array(soup, 'third', 6),
                'fourth_prize': self._extract_prize_array(soup, 'fourth', 4),
                'fifth_prize': self._extract_prize_array(soup, 'fifth', 6),
                'sixth_prize': self._extract_prize_array(soup, 'sixth', 3),
                'seventh_prize': self._extract_prize_array(soup, 'seventh', 4),
            }
            
            # Validate: √≠t nh·∫•t ph·∫£i c√≥ gi·∫£i ƒë·∫∑c bi·ªát
            if results['special_prize']:
                return results
            else:
                print(f"  ‚ö†Ô∏è No special prize found - might be wrong selectors")
                return None
                
        except requests.RequestException as e:
            print(f"  ‚ùå Request error: {e}")
            return None
        except Exception as e:
            print(f"  ‚ùå Parse error: {e}")
            return None
    
    def _crawl_from_minhngoc(self, target_date: date) -> Optional[Dict]:
        """
        Crawl t·ª´ minhngoc.net.vn (backup source)
        
        TODO: Implement parser cho minhngoc.net.vn
        Hi·ªán t·∫°i return None, c·∫ßn update khi c√≥ th·ªùi gian
        """
        # Placeholder - c·∫ßn implement
        print(f"  ‚ö†Ô∏è Backup source not implemented yet")
        return None
    
    def _extract_prize(self, soup: BeautifulSoup, prize_type: str) -> Optional[str]:
        """
        Extract m·ªôt gi·∫£i th∆∞·ªüng ƒë∆°n (ƒêB, Nh·∫•t)
        
        L∆ØU √ù: C·∫ßn update selectors theo c·∫•u tr√∫c th·ª±c t·∫ø c·ªßa website
        """
        try:
            # V√≠ d·ª• selector - C·∫¶N KI·ªÇM TRA L·∫†I
            if prize_type == 'special':
                elem = soup.select_one('.special-prize .number')
            elif prize_type == 'first':
                elem = soup.select_one('.first-prize .number')
            else:
                return None
            
            if elem:
                return elem.text.strip()
            return None
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error extracting {prize_type}: {e}")
            return None
    
    def _extract_prize_array(
        self, 
        soup: BeautifulSoup, 
        prize_type: str, 
        expected_count: int
    ) -> List[str]:
        """
        Extract c√°c gi·∫£i c√≥ nhi·ªÅu s·ªë (Nh√¨, Ba, T∆∞, ...)
        
        Args:
            soup: BeautifulSoup object
            prize_type: Lo·∫°i gi·∫£i ('second', 'third', ...)
            expected_count: S·ªë l∆∞·ª£ng s·ªë d·ª± ki·∫øn
        
        Returns:
            List of strings
        """
        try:
            # V√≠ d·ª• selector - C·∫¶N KI·ªÇM TRA L·∫†I
            selector_map = {
                'second': '.second-prize .number',
                'third': '.third-prize .number',
                'fourth': '.fourth-prize .number',
                'fifth': '.fifth-prize .number',
                'sixth': '.sixth-prize .number',
                'seventh': '.seventh-prize .number',
            }
            
            selector = selector_map.get(prize_type)
            if not selector:
                return []
            
            elements = soup.select(selector)
            numbers = [elem.text.strip() for elem in elements]
            
            # Validate count
            if len(numbers) != expected_count:
                print(f"  ‚ö†Ô∏è Expected {expected_count} numbers for {prize_type}, got {len(numbers)}")
            
            return numbers
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è Error extracting {prize_type} array: {e}")
            return []


def test_crawler():
    """Test crawler v·ªõi ng√†y h√¥m qua"""
    from datetime import timedelta
    
    crawler = XSMBCrawler()
    yesterday = date.today() - timedelta(days=1)
    
    print(f"\n{'='*60}")
    print(f"Testing XSMB Crawler")
    print(f"{'='*60}\n")
    
    results = crawler.fetch_results(yesterday)
    
    if results:
        print(f"\n‚úÖ Crawl successful!")
        print(f"\nResults:")
        print(f"  Date: {results['draw_date']}")
        print(f"  Region: {results['region']}")
        print(f"  Special Prize: {results['special_prize']}")
        print(f"  First Prize: {results['first_prize']}")
        print(f"  Second Prize: {results['second_prize']}")
    else:
        print(f"\n‚ùå Crawl failed!")
        print(f"\n‚ö†Ô∏è L∆ØU √ù:")
        print(f"  1. Ki·ªÉm tra website ngu·ªìn c√≥ ho·∫°t ƒë·ªông kh√¥ng")
        print(f"  2. C·∫ßn update CSS selectors n·∫øu website thay ƒë·ªïi c·∫•u tr√∫c")
        print(f"  3. Th·ª≠ ch·∫°y v·ªõi ng√†y kh√°c (c√≥ th·ªÉ ch∆∞a c√≥ k·∫øt qu·∫£)")


if __name__ == "__main__":
    # Test khi ch·∫°y file n√†y tr·ª±c ti·∫øp
    test_crawler()
