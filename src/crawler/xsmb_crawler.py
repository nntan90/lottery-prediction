"""
XSMB Crawler - MINH NGOC VERSION
Crawl lottery results from minhngoc.net.vn (mien=2)
"""

import requests
from bs4 import BeautifulSoup
from datetime import date
from typing import Optional, Dict
import time

class XSMBCrawler:
    """Crawler for XSMB (Northern Vietnam Lottery) results from Minh Ngoc"""
    
    # XSMB Schedule (Weekday -> Province Slug)
    XSMB_SCHEDULE = {
        0: 'ha-noi',      # Monday
        1: 'quang-ninh',  # Tuesday
        2: 'bac-ninh',    # Wednesday
        3: 'ha-noi',      # Thursday
        4: 'hai-phong',   # Friday
        5: 'nam-dinh',    # Saturday
        6: 'thai-binh'    # Sunday
    }

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                         'AppleWebKit/537.36 (KHTML, like Gecko) '
                         'Chrome/120.0.0.0 Safari/537.36'
        }
    
    def fetch_results(self, target_date: date) -> Optional[Dict]:
        """
        Fetch XSMB results for a specific date
        
        Args:
            target_date: Date to fetch results for
            
        Returns:
            Dictionary with lottery results or None if failed
        """
        try:
            results = self._crawl_from_minhngoc(target_date)
            if results:
                print(f"‚úÖ Successfully crawled XSMB for {target_date}")
                return results
            else:
                print(f"‚ùå No data found for XSMB on {target_date}")
                return None
        except Exception as e:
            print(f"‚ùå Error crawling XSMB for {target_date}: {e}")
            return None
    
    def _clean_prize_list(self, text_value):
        """
        Cleans text extracted with separator='|'.
        Returns a list of strings.
        """
        if not text_value:
            return []
        # Replace common separators with pipe if scraping combined text
        text_value = text_value.replace('-', '|')
        # Split by |
        parts = text_value.split('|')
        # Clean each part
        cleaned = []
        for p in parts:
            p = p.strip()
            # Filter for pure digits (Minh Ngoc might have symbols)
            if p and p.isdigit():
                cleaned.append(p)
        return cleaned

    def _crawl_from_minhngoc(self, target_date: date) -> Optional[Dict]:
        """Crawl from minhngoc.net.vn (Search Interface mien=2)"""
        
        # URL: mien=2 for XSMB
        url = f"https://www.minhngoc.net/tra-cuu-ket-qua-xo-so.html?mien=2&ngay={target_date.day}&thang={target_date.month}&nam={target_date.year}"
        
        print(f"üîç Crawling XSMB: {url}")
        
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            # Check if successful
            if response.status_code != 200:
                print(f"  ‚ùå Failed to fetch: {response.status_code}")
                return None
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Determine table (try refined selector first)
            table = soup.find('table', class_='bkqmienbac')
            if not table:
                # Fallback to general class
                table = soup.find('table', class_='bkqmiennam')
                
            if not table:
                # Check for Holiday
                page_text = soup.get_text().lower()
                if any(kw in page_text for kw in ["ngh·ªâ t·∫øt", "ngh·ªâ l·ªÖ", "l·ªãch t·∫øt", "mi·ªÅn b·∫Øc ngh·ªâ"]):
                    print(f"  ‚ö†Ô∏è Holiday detected: XSMB is not drawn today.")
                    return None

                print(f"  ‚ö†Ô∏è XSMB table (bkqmienbac) not found")
                return None
            
            # Determine Province based on Schedule
            weekday = target_date.weekday()
            province_slug = self.XSMB_SCHEDULE.get(weekday, 'ha-noi') # Fallback
            
            prizes = {}
            
            # Helper to extract
            def extract(class_name, db_field, is_array=True):
                 # Search for ANY td with this class in the table
                td = table.find('td', class_=class_name)
                if td:
                    text = td.get_text(separator='|')
                    values = self._clean_prize_list(text)
                    if not values:
                        return
                    
                    if is_array:
                        prizes[db_field] = values
                    else:
                        prizes[db_field] = values[0]
            
            extract('giaidb', 'special_prize', is_array=False)
            extract('giai1', 'first_prize', is_array=False) 
            extract('giai2', 'second_prize', is_array=True)
            extract('giai3', 'third_prize', is_array=True)
            extract('giai4', 'fourth_prize', is_array=True)
            extract('giai5', 'fifth_prize', is_array=True)
            extract('giai6', 'sixth_prize', is_array=True)
            extract('giai7', 'seventh_prize', is_array=True)
            
            if 'special_prize' not in prizes:
                print(f"  ‚ö†Ô∏è No special prize found for XSMB")
                return None
            
            result = {
                'draw_date': target_date,
                'region': 'XSMB',
                'province': province_slug,
                **prizes
            }
            
            print(f"  ‚úÖ Special Prize: {prizes.get('special_prize')}")
            print(f"  ‚úÖ Province: {province_slug}")
            return result
            
        except Exception as e:
            print(f"  ‚ùå Parse error: {e}")
            import traceback
            traceback.print_exc()
            return None

# Test function
if __name__ == "__main__":
    from datetime import datetime, timedelta
    
    print("=" * 60)
    print("üß™ TESTING XSMB CRAWLER (MINH NGOC VERSION)")
    print("=" * 60)
    
    crawler = XSMBCrawler()
    
    # Test with yesterday (or a specific date)
    # 2024-01-01 was Monday (Hanoi)
    test_date = date(2024, 1, 1) 
    results = crawler.fetch_results(test_date)
    
    if results:
        print("\n‚úÖ SUCCESS! Results for 2024-01-01:")
        for key, value in results.items():
            print(f"  {key}: {value}")
    else:
        print("\n‚ùå FAILED")
